def process_dataset(dataset_path: str, output_dir: str):
    # Load dataset
    with open(dataset_path, "r", encoding="utf-8") as f:
        dataset = json.load(f)

    os.makedirs(output_dir, exist_ok=True)

    for list_id, items in dataset.items():
        list_key = str(list_id)
        output_file = os.path.join(output_dir, f"list_{list_key}.jsonl")

        # Skip if already processed
        if os.path.exists(output_file):
            print(f"Skipping list {list_key} (already exists).")
            continue

        print(f"\n========= Processing List {list_key} =========")

        # We will append JSON objects line by line
        with open(output_file, "w", encoding="utf-8") as out_f:

            with ThreadPoolExecutor(max_workers=MAX_CONCURRENCY) as executor:
                futures = {
                    executor.submit(generate_chat, item["query"], item["trace"]): item
                    for item in items
                }

                for future in as_completed(futures):
                    item = futures[future]
                    query = item["query"]
                    trace = item["trace"]

                    try:
                        answer = future.result()
                    except Exception as e:
                        answer = f"ERROR: {str(e)}"

                    record = {
                        "query": query,
                        "trace": trace,
                        "response": answer
                    }

                    # Write a single json object as one line (JSONL)
                    out_f.write(json.dumps(record, ensure_ascii=False) + "\n")

        print(f"Saved: {output_file}")

    print("\nAll done!")
