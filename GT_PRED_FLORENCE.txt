import torch.nn as nn

def rebuild_detect_head(self, nc: int = 8, reg_max: int = 16):
    """
    Rebuild the Detect head so it predicts `nc` classes while keeping DFL (reg_max) intact.

    Call this after you have self.detect available (the original Detect module)
    and before loading any custom weights that expect the rebuilt head.

    This function:
     - detects the per-scale input channels automatically
     - rebuilds regression (cv2) outputs to reg_max*4 channels
     - rebuilds class (cv3) outputs to nc channels
     - preserves the dfl module (critical)
     - sets detect attributes d.nc, d.reg_max, d.no correctly
    """
    d = self.detect  # original Detect module from ultralytics
    # 1) find channels for each detection scale robustly
    if hasattr(d, "ch") and d.ch is not None:
        channels = list(d.ch)  # common in many versions
    else:
        # fallback: extract from cv2 layer conv input channels
        try:
            channels = [m[0].conv.in_channels for m in d.cv2]
        except Exception:
            # another fallback for slightly different structures
            channels = []
            for m in d.cv2:
                # try common spots
                if hasattr(m[0], "conv") and hasattr(m[0].conv, "in_channels"):
                    channels.append(m[0].conv.in_channels)
                elif hasattr(m[0], "in_channels"):
                    channels.append(m[0].in_channels)
                else:
                    raise RuntimeError("Cannot determine detect cv2 input channels. Inspect d.cv2 structure.")
    # 2) configure core detect attributes
    reg_out = reg_max * 4       # DFL bins * 4 sides (64 for reg_max=16)
    d.nc = nc
    d.reg_max = reg_max
    d.no = reg_out + nc         # training-mode channel count that loss expects (e.g. 64 + 8 = 72)

    # 3) Rebuild regression branches (cv2): output = reg_out
    d.cv2 = nn.ModuleList([
        nn.Sequential(
            nn.Conv2d(c, c, 3, padding=1, bias=False),
            nn.BatchNorm2d(c),
            nn.SiLU(),
            nn.Conv2d(c, c, 3, padding=1, bias=False),
            nn.BatchNorm2d(c),
            nn.SiLU(),
            nn.Conv2d(c, reg_out, 1)
        )
        for c in channels
    ])

    # 4) Rebuild classification branches (cv3): output = nc
    d.cv3 = nn.ModuleList([
        nn.Sequential(
            nn.Conv2d(c, c, 3, padding=1, bias=False),
            nn.BatchNorm2d(c),
            nn.SiLU(),
            nn.Conv2d(c, c, 3, padding=1, bias=False),
            nn.BatchNorm2d(c),
            nn.SiLU(),
            nn.Conv2d(c, nc, 1)
        )
        for c in channels
    ])

    # 5) Keep DFL layer untouched (d.dfl) — critical for decoding/training
    if not hasattr(d, "dfl"):
        # some ultralytics versions put DFL elsewhere – warn but continue
        print("Warning: detect has no attribute dfl; ensure DFL exists or decoding may differ.")

    # 6) Move rebuilt modules to same device as model
    device = next(self.parameters()).device
    d.to(device)

    # 7) print verification
    print("✔ Detect head rebuilt for", nc, "classes")
    print("  cv2 outputs (reg channels):", [m[-1].out_channels for m in d.cv2])
    print("  cv3 outputs (class channels):", [m[-1].out_channels for m in d.cv3])
    print("  training-mode channel count d.no =", d.no, "(= reg_max*4 + nc)")

    # Done (no return). After this, training-mode detect forward should produce (B, d.no, HW)
