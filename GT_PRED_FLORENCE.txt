x = torch.randn(1, 4, 640, 640).to(device)

feats = model.forward_features(x)
print([f.shape for f in feats])

decoded = model.decode(feats)
print("decoded shape:", decoded.shape)

results = model.predict(x)
print("final:", results[0].shape)
print(results[0])


#=====================================================================
from ultralytics.utils.ops import non_max_suppression
import torch
import torch.nn as nn
from ultralytics import YOLO

from ultralytics.utils.nms import non_max_suppression
from ultralytics.utils.tal import make_anchors, dist2bbox

# -------------------------------------------------------------------------
# Your IR FILM Extractor (unchanged)
# -------------------------------------------------------------------------
class IRFiLMExtractor(nn.Module):
    def __init__(self, base_ch=56, out_ch=128):
        super().__init__()

        self.stage1 = nn.Sequential(
            nn.Conv2d(1, base_ch, 3, stride=2, padding=1),
            nn.SiLU()
        )
        self.stage2 = nn.Sequential(
            nn.Conv2d(base_ch, base_ch * 2, 3, stride=2, padding=1),
            nn.SiLU()
        )
        self.stage3 = nn.Sequential(
            nn.Conv2d(base_ch * 2, base_ch * 4, 3, stride=2, padding=1),
            nn.SiLU()
        )
        self.stage4 = nn.Sequential(
            nn.Conv2d(base_ch * 4, base_ch * 8, 3, stride=2, padding=1),
            nn.SiLU()
        )

        self.to_gamma = nn.Conv2d(base_ch * 8, out_ch, 1)
        self.to_beta  = nn.Conv2d(base_ch * 8, out_ch, 1)

    def forward(self, ir):
        f1 = self.stage1(ir)
        f2 = self.stage2(f1)
        f3 = self.stage3(f2)
        f4 = self.stage4(f3)

        gamma = self.to_gamma(f4)
        beta = self.to_beta(f4)
        return gamma, beta


# -------------------------------------------------------------------------
# Updated CustomYOLO with IRFiLMExtractor
# -------------------------------------------------------------------------
class CustomYOLO(nn.Module):
    def __init__(self, weights_path: str):
        super().__init__()

        base = YOLO(weights_path).model
        self.model = base
        self.layers = base.model
        self.detect = self.layers[-1]
        self.nc = base.nc
        
        # --- Get P5 channel count automatically ---
        p5_channels = self.layers[9].cv2.conv.out_channels

        # --- Replace FiLM with your IRFiLMExtractor ---
        self.film = IRFiLMExtractor(base_ch=56, out_ch=p5_channels)

    # ------------------------------------------------------------------
    # Backbone + Neck (feature extraction only)
    # ------------------------------------------------------------------
    def forward_features(self, x4):
        rgb = x4[:, :3]
        ir = x4[:, 3:].unsqueeze(1)

        xi = rgb
        P3 = P4 = P5 = None
        layers = self.layers

        # -- BACKBONE --
        for i, m in enumerate(layers):
            xi = m(xi)
            if i == 4: P3 = xi
            if i == 6: P4 = xi
            if i == 9:
                P5 = xi
                break

        # --- IR FiLM Conditioning ---
        gamma, beta = self.film(ir)
        P5 = P5 * gamma + beta

        # -- NECK --
        P5_up  = layers[10](P5)
        F4     = layers[11]([P5_up, P4])
        P4_out = layers[12](F4)

        P4_up  = layers[13](P4_out)
        F3     = layers[14]([P4_up, P3])
        P3_out = layers[15](F3)

        P3_down = layers[16](P3_out)
        F4b     = layers[17]([P3_down, P4_out])
        P4b     = layers[18](F4b)

        P4_down = layers[19](P4b)
        F5b     = layers[20]([P4_down, P5])
        P5b     = layers[21](F5b)

        return [P3_out, P4b, P5b]

    # ------------------------------------------------------------------
    # TRAINING FORWARD
    # ------------------------------------------------------------------
    def forward(self, x4):
        feats = self.forward_features(x4)
        return self.detect(feats)   # raw YOLO detect output

    # ------------------------------------------------------------------
    # DECODE + NMS
    # ------------------------------------------------------------------
    @torch.no_grad()
    def predict(self, x4, conf=0.25, iou=0.45):
        feats = self.forward_features(x4)
        decoded = self.decode(feats)
        return non_max_suppression(decoded, conf, iou, nc=self.nc)

    # ------------------------------------------------------------------
    # YOLOv8 DFL + anchor decoding
    # ------------------------------------------------------------------
    @torch.no_grad()
    def decode(self, feats):
        detect = self.detect

        stride = detect.stride
        reg_max = detect.reg_max
        nc = detect.nc
        no = nc + reg_max * 4
        device = feats[0].device

        anchor_points, stride_tensor = make_anchors(feats, stride, 0.5)

        B = feats[0].shape[0]
        feats = [f.view(B, no, -1) for f in feats]

        pred_distri, pred_scores = torch.cat(feats, dim=2).split((reg_max * 4, nc), 1)

        pred_distri = pred_distri.permute(0, 2, 1)
        pred_scores = pred_scores.permute(0, 2, 1).sigmoid()

        proj = torch.arange(reg_max, device=device)
        pred_dist = pred_distri.view(B, -1, 4, reg_max).softmax(3)
        pred_dist = pred_dist.matmul(proj)

        boxes = dist2bbox(pred_dist, anchor_points, xywh=False)
        boxes *= stride_tensor

        return torch.cat([boxes, pred_scores], dim=-1)



#====================================================================

from ultralytics.utils.metrics import DetMetrics


@torch.no_grad()
def validate(model, dataloader, device, conf=0.25, iou=0.45):
    model.eval()

    metrics = DetMetrics()
    seen = 0

    for imgs, targets in dataloader:
        imgs = imgs.to(device)
        preds = model.infer(imgs, conf=conf, iou=iou)

        B = imgs.shape[0]
        seen += B

        # ----------------------------------------
        # Convert GT labels to YOLO expected format
        # ----------------------------------------
        gt = []
        index = 0
        for i, t in enumerate(targets):
            if len(t) == 0:
                continue

            for obj in t:
                cls = int(obj[0])
                x, y, w, h = obj[1:].tolist()

                # Convert normalized xywh â†’ xyxy absolute pixels
                W = imgs.shape[3]
                H = imgs.shape[2]
                x1 = (x - w/2) * W
                y1 = (y - h/2) * H
                x2 = (x + w/2) * W
                y2 = (y + h/2) * H

                gt.append([i, cls, x1, y1, x2, y2])
            index += 1

        if len(gt) == 0:
            continue

        gt = torch.tensor(gt, device=device)

        # ----------------------------------------
        # Update metrics
        # ----------------------------------------
        metrics.update(preds, gt)

    final = metrics.compute(seen)
    return final


for epoch in range(epochs):
    train_one_epoch()
    
    val_results = validate(model, val_loader, device)
    print(f"Epoch {epoch+1}/{epochs}")
    print(f" Precision:   {val_results['precision']:.4f}")
    print(f" Recall:      {val_results['recall']:.4f}")
    print(f" mAP@50:      {val_results['map50']:.4f}")
    print(f" mAP@50-95:   {val_results['map']:.4f}")





